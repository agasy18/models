{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = '/hdd/datasets/mscoco/dataset/mscoco/train2014/'\n",
    "images = [os.path.join(images_dir, f) for f in os.listdir(images_dir)]\n",
    "\n",
    "image_height = 299\n",
    "image_width = 299\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _depthwise_separable_conv(inputs,\n",
    "                                  num_pwc_filters,\n",
    "                                  width_multiplier,\n",
    "                                  sc,\n",
    "                                  stride=1):\n",
    "        \"\"\" Helper function to build the depth-wise separable convolution layer.\n",
    "      \"\"\"\n",
    "        num_pwc_filters = round(num_pwc_filters * width_multiplier)\n",
    "        from tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3_base\n",
    "\n",
    "        slim = tf.contrib.slim\n",
    "\n",
    "        # skip pointwise by setting num_outputs=None\n",
    "        depthwise_conv = slim.separable_convolution2d(inputs,\n",
    "                                                      num_outputs=None,\n",
    "                                                      stride=stride,\n",
    "                                                      depth_multiplier=1,\n",
    "                                                      kernel_size=[3, 3],\n",
    "                                                      scope=sc + '/depthwise_conv')\n",
    "\n",
    "        bn = slim.batch_norm(depthwise_conv, scope=sc + '/dw_batch_norm')\n",
    "        pointwise_conv = slim.convolution2d(bn,\n",
    "                                            num_pwc_filters,\n",
    "                                            kernel_size=[1, 1],\n",
    "                                            scope=sc + '/pointwise_conv')\n",
    "        return slim.batch_norm(pointwise_conv, scope=sc + '/pw_batch_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssd_model_file = 'ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb'\n",
    "if not os.path.isfile(ssd_model_file):\n",
    "    from subprocess import call\n",
    "    url = 'http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz'\n",
    "    call(['wget', '-nc', url])\n",
    "    tar = 'ssd_mobilenet_v1_coco_2017_11_17.tar.gz'\n",
    "    call(['tar', '-xf', tar, '-C', './'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_graph_def = tf.GraphDef()\n",
    "with open('/hdd/models/im2txt_2016_10_05.1000000/const_model.ckpt-1000000.pb', 'rb') as f:\n",
    "    orig_graph_def.ParseFromString(f.read())\n",
    "orig_image_embedding_node_name = 'image_embedding/image_embedding/MatMul'\n",
    "orig_graph_def=tf.graph_util.extract_sub_graph(orig_graph_def, [orig_image_embedding_node_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29112121"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssd_graph_def = tf.GraphDef()\n",
    "with tf.gfile.GFile(ssd_model_file, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "ssd_graph_def.ParseFromString(serialized_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(images)\n",
    "def map_to_image(p):\n",
    "    pc = tf.read_file(p)\n",
    "    img = tf.image.decode_jpeg(pc, 3)\n",
    "    img = tf.image.resize_images(img, (image_height, image_width))\n",
    "    return {'images': tf.cast(img, dtype=tf.uint8), 'keepThis': tf.shape(img)[-1] == 3}, {'images': img * (2.0/255) - 1.0}\n",
    "dataset = dataset.map(map_to_image, 8)\n",
    "# dataset = dataset.filter(lambda f, l: f['keepThis'])\n",
    "# dataset = dataset.cache('/hdd/tmp/only-ssd-cache')\n",
    "dataset = dataset.repeat(100000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    tf.import_graph_def(graph_def=orig_graph_def, input_map={'ExpandDims_4': labels['images']}, name='orig')\n",
    "    orig_image_embedding = tf.get_default_graph().get_tensor_by_name(\"orig/\" + orig_image_embedding_node_name + ':0')\n",
    "\n",
    "    feature_layers = ['FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/Relu6:0']\n",
    "\n",
    "    res = tf.import_graph_def(ssd_graph_def, name='', input_map={'image_tensor:0':features['images']}, return_elements=feature_layers)\n",
    "    \n",
    "    net = res[0]\n",
    "\n",
    "    width_multiplier = 1\n",
    "    net = _depthwise_separable_conv(net, 1024, width_multiplier, stride=2, sc='x_conv_ds_11')\n",
    "    net = _depthwise_separable_conv(net, 1024, width_multiplier, stride=3, sc='x_conv_ds_12')\n",
    "    net = _depthwise_separable_conv(net, 512, width_multiplier, stride=2, sc='x_conv_ds_13')\n",
    "\n",
    "    net = tf.layers.flatten(net)\n",
    "\n",
    "    net = tf.layers.dense(net, 1024, activation=tf.nn.sigmoid)\n",
    "\n",
    "    image_embeddings = tf.layers.dense(net, 512)\n",
    "    \n",
    "\n",
    "    tf.summary.histogram('image_embedding/orig', orig_image_embedding)\n",
    "    tf.summary.histogram('image_embedding/self', image_embeddings)\n",
    "    tf.summary.histogram('image_embedding/diff', orig_image_embedding - image_embeddings)\n",
    "\n",
    "    total_loss = tf.losses.mean_squared_error(orig_image_embedding ,image_embeddings)\n",
    "    tf.summary.scalar(\"losses/copy\", total_loss)\n",
    "    \n",
    "    global_step = tf.train.get_global_step()\n",
    "    learning_rate = params['learning_rate']\n",
    "    loss = tf.losses.get_total_loss()\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    tf.summary.scalar('batch_size', tf.shape(image_embeddings)[0])\n",
    "    train_op = tf.train.AdagradOptimizer(learning_rate,\n",
    "        params['learning_rate']).minimize(loss, global_step=global_step)\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode, \n",
    "#         predictions=predictions,\n",
    "        loss=loss,\n",
    "        train_op=train_op\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 30, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5e4d7e0cd0>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/hdd/train/only-ssd/34', '_train_distribute': None, '_save_summary_steps': 100}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f5e4d7e0c50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = '/hdd/train/only-ssd/'\n",
    "model_dir += str(len(os.listdir(model_dir)))\n",
    "os.path.exists(model_dir) or os.makedirs(model_dir)\n",
    "estimator = tf.estimator.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    model_dir=model_dir, \n",
    "    params={\n",
    "        'learning_rate': 0.05,\n",
    "    }, \n",
    "    warm_start_from=tf.estimator.WarmStartSettings('/hdd/train/only-ssd/32'),\n",
    "    config=tf.estimator.RunConfig(log_step_count_steps=100, keep_checkpoint_max=30))\n",
    "estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/hdd/train/only-ssd/32', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "INFO:tensorflow:Warm-starting from: ('/hdd/train/only-ssd/32',)\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_11/dw_batch_norm/beta; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_11/pointwise_conv/biases; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_13/depthwise_conv/biases; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_11/depthwise_conv/depthwise_weights; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_12/pointwise_conv/biases; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_13/pw_batch_norm/beta; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_11/pointwise_conv/weights; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_11/depthwise_conv/biases; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_13/dw_batch_norm/beta; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_12/pw_batch_norm/beta; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_12/depthwise_conv/depthwise_weights; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_11/pw_batch_norm/beta; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_12/depthwise_conv/biases; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_13/pointwise_conv/weights; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_12/pointwise_conv/weights; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_13/pointwise_conv/biases; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_12/dw_batch_norm/beta; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: x_conv_ds_13/depthwise_conv/depthwise_weights; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /hdd/train/only-ssd/34/model.ckpt.\n",
      "INFO:tensorflow:loss = 19.208015, step = 0\n",
      "INFO:tensorflow:global_step/sec: 22.1968\n",
      "INFO:tensorflow:loss = 26.787289, step = 100 (4.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6359\n",
      "INFO:tensorflow:loss = 21.469841, step = 200 (2.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6635\n",
      "INFO:tensorflow:loss = 18.109404, step = 300 (2.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6478\n",
      "INFO:tensorflow:loss = 22.093483, step = 400 (2.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6084\n",
      "INFO:tensorflow:loss = 20.798658, step = 500 (2.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6872\n",
      "INFO:tensorflow:loss = 24.756865, step = 600 (2.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6399\n",
      "INFO:tensorflow:loss = 25.838257, step = 700 (2.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6613\n",
      "INFO:tensorflow:loss = 15.171289, step = 800 (2.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.651\n",
      "INFO:tensorflow:loss = 16.71772, step = 900 (2.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6345\n",
      "INFO:tensorflow:loss = 22.057947, step = 1000 (2.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.5857\n",
      "INFO:tensorflow:loss = 14.533892, step = 1100 (2.405 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.5781\n",
      "INFO:tensorflow:loss = 18.035524, step = 1200 (2.405 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.3651\n",
      "INFO:tensorflow:loss = 14.466086, step = 1300 (2.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.1685\n",
      "INFO:tensorflow:loss = 16.0484, step = 1400 (2.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.3581\n",
      "INFO:tensorflow:loss = 12.36643, step = 1500 (2.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6563\n",
      "INFO:tensorflow:loss = 9.871683, step = 1600 (2.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.5589\n",
      "INFO:tensorflow:loss = 30.528173, step = 1700 (2.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6591\n",
      "INFO:tensorflow:loss = 9.223516, step = 1800 (2.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6203\n",
      "INFO:tensorflow:loss = 20.615164, step = 1900 (2.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6753\n",
      "INFO:tensorflow:loss = 15.979864, step = 2000 (2.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.4857\n",
      "INFO:tensorflow:loss = 12.312945, step = 2100 (2.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.3352\n",
      "INFO:tensorflow:loss = 5.569903, step = 2200 (2.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.3703\n",
      "INFO:tensorflow:loss = 10.214686, step = 2300 (2.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.73\n",
      "INFO:tensorflow:loss = 6.1731486, step = 2400 (2.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.656\n",
      "INFO:tensorflow:loss = 19.487885, step = 2500 (2.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6273\n",
      "INFO:tensorflow:loss = 9.145893, step = 2600 (2.402 sec)\n"
     ]
    }
   ],
   "source": [
    "for batch_size in range(2, 1000, 2):\n",
    "    step_count = len(images) / batch_size\n",
    "    estimator.train(input_fn=lambda: dataset.batch(batch_size, drop_remainder=True).make_one_shot_iterator().get_next(), steps=step_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
